{"ast":null,"code":"import _objectSpread from \"/Users/victor/Documents/Sublime/claude_api_project/chat-pwa/node_modules/@babel/runtime/helpers/esm/objectSpread2.js\";\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../resource.mjs\";\nimport { isRequestOptions } from \"../core.mjs\";\nimport { CursorPage } from \"../pagination.mjs\";\nexport class Batches extends APIResource {\n  /**\n   * Creates and executes a batch from an uploaded file of requests\n   */\n  create(body, options) {\n    return this._client.post('/batches', _objectSpread({\n      body\n    }, options));\n  }\n  /**\n   * Retrieves a batch.\n   */\n  retrieve(batchId, options) {\n    return this._client.get(\"/batches/\".concat(batchId), options);\n  }\n  list() {\n    let query = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    let options = arguments.length > 1 ? arguments[1] : undefined;\n    if (isRequestOptions(query)) {\n      return this.list({}, query);\n    }\n    return this._client.getAPIList('/batches', BatchesPage, _objectSpread({\n      query\n    }, options));\n  }\n  /**\n   * Cancels an in-progress batch. The batch will be in status `cancelling` for up to\n   * 10 minutes, before changing to `cancelled`, where it will have partial results\n   * (if any) available in the output file.\n   */\n  cancel(batchId, options) {\n    return this._client.post(\"/batches/\".concat(batchId, \"/cancel\"), options);\n  }\n}\nexport class BatchesPage extends CursorPage {}\nBatches.BatchesPage = BatchesPage;","map":{"version":3,"names":["APIResource","isRequestOptions","CursorPage","Batches","create","body","options","_client","post","_objectSpread","retrieve","batchId","get","concat","list","query","arguments","length","undefined","getAPIList","BatchesPage","cancel"],"sources":["/Users/victor/Documents/Sublime/claude_api_project/chat-pwa/node_modules/openai/src/resources/batches.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../resource';\nimport { isRequestOptions } from '../core';\nimport * as Core from '../core';\nimport * as BatchesAPI from './batches';\nimport { CursorPage, type CursorPageParams } from '../pagination';\n\nexport class Batches extends APIResource {\n  /**\n   * Creates and executes a batch from an uploaded file of requests\n   */\n  create(body: BatchCreateParams, options?: Core.RequestOptions): Core.APIPromise<Batch> {\n    return this._client.post('/batches', { body, ...options });\n  }\n\n  /**\n   * Retrieves a batch.\n   */\n  retrieve(batchId: string, options?: Core.RequestOptions): Core.APIPromise<Batch> {\n    return this._client.get(`/batches/${batchId}`, options);\n  }\n\n  /**\n   * List your organization's batches.\n   */\n  list(query?: BatchListParams, options?: Core.RequestOptions): Core.PagePromise<BatchesPage, Batch>;\n  list(options?: Core.RequestOptions): Core.PagePromise<BatchesPage, Batch>;\n  list(\n    query: BatchListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<BatchesPage, Batch> {\n    if (isRequestOptions(query)) {\n      return this.list({}, query);\n    }\n    return this._client.getAPIList('/batches', BatchesPage, { query, ...options });\n  }\n\n  /**\n   * Cancels an in-progress batch. The batch will be in status `cancelling` for up to\n   * 10 minutes, before changing to `cancelled`, where it will have partial results\n   * (if any) available in the output file.\n   */\n  cancel(batchId: string, options?: Core.RequestOptions): Core.APIPromise<Batch> {\n    return this._client.post(`/batches/${batchId}/cancel`, options);\n  }\n}\n\nexport class BatchesPage extends CursorPage<Batch> {}\n\nexport interface Batch {\n  id: string;\n\n  /**\n   * The time frame within which the batch should be processed.\n   */\n  completion_window: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch was created.\n   */\n  created_at: number;\n\n  /**\n   * The OpenAI API endpoint used by the batch.\n   */\n  endpoint: string;\n\n  /**\n   * The ID of the input file for the batch.\n   */\n  input_file_id: string;\n\n  /**\n   * The object type, which is always `batch`.\n   */\n  object: 'batch';\n\n  /**\n   * The current status of the batch.\n   */\n  status:\n    | 'validating'\n    | 'failed'\n    | 'in_progress'\n    | 'finalizing'\n    | 'completed'\n    | 'expired'\n    | 'cancelling'\n    | 'cancelled';\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch was cancelled.\n   */\n  cancelled_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch started cancelling.\n   */\n  cancelling_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch was completed.\n   */\n  completed_at?: number;\n\n  /**\n   * The ID of the file containing the outputs of requests with errors.\n   */\n  error_file_id?: string;\n\n  errors?: Batch.Errors;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch expired.\n   */\n  expired_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch will expire.\n   */\n  expires_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch failed.\n   */\n  failed_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch started finalizing.\n   */\n  finalizing_at?: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the batch started processing.\n   */\n  in_progress_at?: number;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n\n  /**\n   * The ID of the file containing the outputs of successfully executed requests.\n   */\n  output_file_id?: string;\n\n  /**\n   * The request counts for different statuses within the batch.\n   */\n  request_counts?: BatchRequestCounts;\n}\n\nexport namespace Batch {\n  export interface Errors {\n    data?: Array<BatchesAPI.BatchError>;\n\n    /**\n     * The object type, which is always `list`.\n     */\n    object?: string;\n  }\n}\n\nexport interface BatchError {\n  /**\n   * An error code identifying the error type.\n   */\n  code?: string;\n\n  /**\n   * The line number of the input file where the error occurred, if applicable.\n   */\n  line?: number | null;\n\n  /**\n   * A human-readable message providing more details about the error.\n   */\n  message?: string;\n\n  /**\n   * The name of the parameter that caused the error, if applicable.\n   */\n  param?: string | null;\n}\n\n/**\n * The request counts for different statuses within the batch.\n */\nexport interface BatchRequestCounts {\n  /**\n   * Number of requests that have been completed successfully.\n   */\n  completed: number;\n\n  /**\n   * Number of requests that have failed.\n   */\n  failed: number;\n\n  /**\n   * Total number of requests in the batch.\n   */\n  total: number;\n}\n\nexport interface BatchCreateParams {\n  /**\n   * The time frame within which the batch should be processed. Currently only `24h`\n   * is supported.\n   */\n  completion_window: '24h';\n\n  /**\n   * The endpoint to be used for all requests in the batch. Currently\n   * `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions` are supported.\n   * Note that `/v1/embeddings` batches are also restricted to a maximum of 50,000\n   * embedding inputs across all requests in the batch.\n   */\n  endpoint: '/v1/chat/completions' | '/v1/embeddings' | '/v1/completions';\n\n  /**\n   * The ID of an uploaded file that contains requests for the new batch.\n   *\n   * See [upload file](https://platform.openai.com/docs/api-reference/files/create)\n   * for how to upload a file.\n   *\n   * Your input file must be formatted as a\n   * [JSONL file](https://platform.openai.com/docs/api-reference/batch/request-input),\n   * and must be uploaded with the purpose `batch`. The file can contain up to 50,000\n   * requests, and can be up to 200 MB in size.\n   */\n  input_file_id: string;\n\n  /**\n   * Optional custom metadata for the batch.\n   */\n  metadata?: Record<string, string> | null;\n}\n\nexport interface BatchListParams extends CursorPageParams {}\n\nBatches.BatchesPage = BatchesPage;\n\nexport declare namespace Batches {\n  export {\n    type Batch as Batch,\n    type BatchError as BatchError,\n    type BatchRequestCounts as BatchRequestCounts,\n    BatchesPage as BatchesPage,\n    type BatchCreateParams as BatchCreateParams,\n    type BatchListParams as BatchListParams,\n  };\n}\n"],"mappings":";AAAA;SAESA,WAAW,QAAE;SACbC,gBAAgB,QAAE;SAGlBC,UAAU,QAAyB;AAE5C,OAAM,MAAOC,OAAQ,SAAQH,WAAW;EACtC;;;EAGAI,MAAMA,CAACC,IAAuB,EAAEC,OAA6B;IAC3D,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,UAAU,EAAAC,aAAA;MAAIJ;IAAI,GAAKC,OAAO,CAAE,CAAC;EAC5D;EAEA;;;EAGAI,QAAQA,CAACC,OAAe,EAAEL,OAA6B;IACrD,OAAO,IAAI,CAACC,OAAO,CAACK,GAAG,aAAAC,MAAA,CAAaF,OAAO,GAAIL,OAAO,CAAC;EACzD;EAOAQ,IAAIA,CAAA,EAE2B;IAAA,IAD7BC,KAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAA+C,EAAE;IAAA,IACjDV,OAA6B,GAAAU,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAE,SAAA;IAE7B,IAAIjB,gBAAgB,CAACc,KAAK,CAAC,EAAE;MAC3B,OAAO,IAAI,CAACD,IAAI,CAAC,EAAE,EAAEC,KAAK,CAAC;;IAE7B,OAAO,IAAI,CAACR,OAAO,CAACY,UAAU,CAAC,UAAU,EAAEC,WAAW,EAAAX,aAAA;MAAIM;IAAK,GAAKT,OAAO,CAAE,CAAC;EAChF;EAEA;;;;;EAKAe,MAAMA,CAACV,OAAe,EAAEL,OAA6B;IACnD,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,aAAAK,MAAA,CAAaF,OAAO,cAAWL,OAAO,CAAC;EACjE;;AAGF,OAAM,MAAOc,WAAY,SAAQlB,UAAiB;AAsMlDC,OAAO,CAACiB,WAAW,GAAGA,WAAW","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}