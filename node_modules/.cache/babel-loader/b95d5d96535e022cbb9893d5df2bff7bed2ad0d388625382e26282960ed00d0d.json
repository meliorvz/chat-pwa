{"ast":null,"code":"import _objectSpread from \"/Users/victor/Documents/Sublime/claude_api_project/chat-pwa/node_modules/@babel/runtime/helpers/esm/objectSpread2.js\";\n// File generated from our OpenAPI spec by Stainless.\nimport { APIResource } from '@anthropic-ai/sdk/resource';\nimport { MessageStream } from '@anthropic-ai/sdk/lib/MessageStream';\nexport { MessageStream } from '@anthropic-ai/sdk/lib/MessageStream';\nexport class Messages extends APIResource {\n  create(body, options) {\n    var _body$stream;\n    return this._client.post('/v1/messages', _objectSpread(_objectSpread({\n      body,\n      timeout: 600000\n    }, options), {}, {\n      stream: (_body$stream = body.stream) !== null && _body$stream !== void 0 ? _body$stream : false\n    }));\n  }\n  /**\n   * Create a Message stream\n   */\n  stream(body, options) {\n    return MessageStream.createMessage(this, body, options);\n  }\n}\n(function (Messages) {})(Messages || (Messages = {}));","map":{"version":3,"names":["APIResource","MessageStream","Messages","create","body","options","_body$stream","_client","post","_objectSpread","timeout","stream","createMessage"],"sources":["/Users/victor/Documents/Sublime/claude_api_project/chat-pwa/node_modules/@anthropic-ai/sdk/src/resources/messages.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../core\";\nimport { APIPromise } from \"../core\";\nimport { APIResource } from \"../resource\";\nimport { MessageStream } from \"../lib/MessageStream\";\nexport { MessageStream } from \"../lib/MessageStream\";\nimport * as MessagesAPI from \"./messages\";\nimport { Stream } from \"../streaming\";\n\nexport class Messages extends APIResource {\n  /**\n   * Create a Message.\n   *\n   * Send a structured list of input messages with text and/or image content, and the\n   * model will generate the next message in the conversation.\n   *\n   * The Messages API can be used for for either single queries or stateless\n   * multi-turn conversations.\n   */\n  create(body: MessageCreateParamsNonStreaming, options?: Core.RequestOptions): APIPromise<Message>;\n  create(\n    body: MessageCreateParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<MessageStreamEvent>>;\n  create(\n    body: MessageCreateParamsBase,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<MessageStreamEvent> | Message>;\n  create(\n    body: MessageCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<Message> | APIPromise<Stream<MessageStreamEvent>> {\n    return this._client.post('/v1/messages', {\n      body,\n      timeout: 600000,\n      ...options,\n      stream: body.stream ?? false,\n    }) as APIPromise<Message> | APIPromise<Stream<MessageStreamEvent>>;\n  }\n\n  /**\n   * Create a Message stream\n   */\n  stream(body: MessageStreamParams, options?: Core.RequestOptions): MessageStream {\n    return MessageStream.createMessage(this, body, options);\n  }\n}\n\nexport interface ContentBlock {\n  text: string;\n\n  type: 'text';\n}\n\nexport interface ContentBlockDeltaEvent {\n  delta: TextDelta;\n\n  index: number;\n\n  type: 'content_block_delta';\n}\n\nexport interface ContentBlockStartEvent {\n  content_block: ContentBlock;\n\n  index: number;\n\n  type: 'content_block_start';\n}\n\nexport interface ContentBlockStopEvent {\n  index: number;\n\n  type: 'content_block_stop';\n}\n\nexport interface ImageBlockParam {\n  source: ImageBlockParam.Source;\n\n  type?: 'image';\n}\n\nexport namespace ImageBlockParam {\n  export interface Source {\n    data: string;\n\n    media_type: 'image/jpeg' | 'image/png' | 'image/gif' | 'image/webp';\n\n    type?: 'base64';\n  }\n}\n\nexport interface Message {\n  /**\n   * Unique object identifier.\n   *\n   * The format and length of IDs may change over time.\n   */\n  id: string;\n\n  /**\n   * Content generated by the model.\n   *\n   * This is an array of content blocks, each of which has a `type` that determines\n   * its shape. Currently, the only `type` in responses is `\"text\"`.\n   *\n   * Example:\n   *\n   * ```json\n   * [{ \"type\": \"text\", \"text\": \"Hi, I'm Claude.\" }]\n   * ```\n   *\n   * If the request input `messages` ended with an `assistant` turn, then the\n   * response `content` will continue directly from that last turn. You can use this\n   * to constrain the model's output.\n   *\n   * For example, if the input `messages` were:\n   *\n   * ```json\n   * [\n   *   {\n   *     \"role\": \"user\",\n   *     \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"\n   *   },\n   *   { \"role\": \"assistant\", \"content\": \"The best answer is (\" }\n   * ]\n   * ```\n   *\n   * Then the response `content` might be:\n   *\n   * ```json\n   * [{ \"type\": \"text\", \"text\": \"B)\" }]\n   * ```\n   */\n  content: Array<ContentBlock>;\n\n  /**\n   * The model that handled the request.\n   */\n  model: string;\n\n  /**\n   * Conversational role of the generated message.\n   *\n   * This will always be `\"assistant\"`.\n   */\n  role: 'assistant';\n\n  /**\n   * The reason that we stopped.\n   *\n   * This may be one the following values:\n   *\n   * - `\"end_turn\"`: the model reached a natural stopping point\n   * - `\"max_tokens\"`: we exceeded the requested `max_tokens` or the model's maximum\n   * - `\"stop_sequence\"`: one of your provided custom `stop_sequences` was generated\n   *\n   * Note that these values are different than those in `/v1/complete`, where\n   * `end_turn` and `stop_sequence` were not differentiated.\n   *\n   * In non-streaming mode this value is always non-null. In streaming mode, it is\n   * null in the `message_start` event and non-null otherwise.\n   */\n  stop_reason: 'end_turn' | 'max_tokens' | 'stop_sequence' | null;\n\n  /**\n   * Which custom stop sequence was generated, if any.\n   *\n   * This value will be a non-null string if one of your custom stop sequences was\n   * generated.\n   */\n  stop_sequence: string | null;\n\n  /**\n   * Object type.\n   *\n   * For Messages, this is always `\"message\"`.\n   */\n  type: 'message';\n\n  /**\n   * Billing and rate-limit usage.\n   *\n   * Anthropic's API bills and rate-limits by token counts, as tokens represent the\n   * underlying cost to our systems.\n   *\n   * Under the hood, the API transforms requests into a format suitable for the\n   * model. The model's output then goes through a parsing stage before becoming an\n   * API response. As a result, the token counts in `usage` will not match one-to-one\n   * with the exact visible content of an API request or response.\n   *\n   * For example, `output_tokens` will be non-zero, even for an empty string response\n   * from Claude.\n   */\n  usage: Usage;\n}\n\nexport interface MessageDeltaEvent {\n  delta: MessageDeltaEvent.Delta;\n\n  type: 'message_delta';\n\n  /**\n   * Billing and rate-limit usage.\n   *\n   * Anthropic's API bills and rate-limits by token counts, as tokens represent the\n   * underlying cost to our systems.\n   *\n   * Under the hood, the API transforms requests into a format suitable for the\n   * model. The model's output then goes through a parsing stage before becoming an\n   * API response. As a result, the token counts in `usage` will not match one-to-one\n   * with the exact visible content of an API request or response.\n   *\n   * For example, `output_tokens` will be non-zero, even for an empty string response\n   * from Claude.\n   */\n  usage: MessageDeltaUsage;\n}\n\nexport namespace MessageDeltaEvent {\n  export interface Delta {\n    stop_reason: 'end_turn' | 'max_tokens' | 'stop_sequence' | null;\n\n    stop_sequence: string | null;\n  }\n}\n\nexport interface MessageDeltaUsage {\n  /**\n   * The cumulative number of output tokens which were used.\n   */\n  output_tokens: number;\n}\n\nexport interface MessageParam {\n  content: string | Array<TextBlock | ImageBlockParam>;\n\n  role: 'user' | 'assistant';\n}\n\nexport interface MessageStartEvent {\n  message: Message;\n\n  type: 'message_start';\n}\n\nexport interface MessageStopEvent {\n  type: 'message_stop';\n}\n\nexport type MessageStreamEvent =\n  | MessageStartEvent\n  | MessageDeltaEvent\n  | MessageStopEvent\n  | ContentBlockStartEvent\n  | ContentBlockDeltaEvent\n  | ContentBlockStopEvent;\n\nexport interface TextBlock {\n  text: string;\n\n  type?: 'text';\n}\n\nexport interface TextDelta {\n  text: string;\n\n  type: 'text_delta';\n}\n\nexport interface Usage {\n  /**\n   * The number of input tokens which were used.\n   */\n  input_tokens: number;\n\n  /**\n   * The number of output tokens which were used.\n   */\n  output_tokens: number;\n}\n\nexport type MessageCreateParams = MessageCreateParamsNonStreaming | MessageCreateParamsStreaming;\n\nexport interface MessageCreateParamsBase {\n  /**\n   * The maximum number of tokens to generate before stopping.\n   *\n   * Note that our models may stop _before_ reaching this maximum. This parameter\n   * only specifies the absolute maximum number of tokens to generate.\n   *\n   * Different models have different maximum values for this parameter. See\n   * [models](https://docs.anthropic.com/claude/docs/models-overview) for details.\n   */\n  max_tokens: number;\n\n  /**\n   * Input messages.\n   *\n   * Our models are trained to operate on alternating `user` and `assistant`\n   * conversational turns. When creating a new `Message`, you specify the prior\n   * conversational turns with the `messages` parameter, and the model then generates\n   * the next `Message` in the conversation.\n   *\n   * Each input message must be an object with a `role` and `content`. You can\n   * specify a single `user`-role message, or you can include multiple `user` and\n   * `assistant` messages. The first message must always use the `user` role.\n   *\n   * If the final message uses the `assistant` role, the response content will\n   * continue immediately from the content in that message. This can be used to\n   * constrain part of the model's response.\n   *\n   * Example with a single `user` message:\n   *\n   * ```json\n   * [{ \"role\": \"user\", \"content\": \"Hello, Claude\" }]\n   * ```\n   *\n   * Example with multiple conversational turns:\n   *\n   * ```json\n   * [\n   *   { \"role\": \"user\", \"content\": \"Hello there.\" },\n   *   { \"role\": \"assistant\", \"content\": \"Hi, I'm Claude. How can I help you?\" },\n   *   { \"role\": \"user\", \"content\": \"Can you explain LLMs in plain English?\" }\n   * ]\n   * ```\n   *\n   * Example with a partially-filled response from Claude:\n   *\n   * ```json\n   * [\n   *   {\n   *     \"role\": \"user\",\n   *     \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"\n   *   },\n   *   { \"role\": \"assistant\", \"content\": \"The best answer is (\" }\n   * ]\n   * ```\n   *\n   * Each input message `content` may be either a single `string` or an array of\n   * content blocks, where each block has a specific `type`. Using a `string` for\n   * `content` is shorthand for an array of one content block of type `\"text\"`. The\n   * following input messages are equivalent:\n   *\n   * ```json\n   * { \"role\": \"user\", \"content\": \"Hello, Claude\" }\n   * ```\n   *\n   * ```json\n   * { \"role\": \"user\", \"content\": [{ \"type\": \"text\", \"text\": \"Hello, Claude\" }] }\n   * ```\n   *\n   * Starting with Claude 3 models, you can also send image content blocks:\n   *\n   * ```json\n   * {\n   *   \"role\": \"user\",\n   *   \"content\": [\n   *     {\n   *       \"type\": \"image\",\n   *       \"source\": {\n   *         \"type\": \"base64\",\n   *         \"media_type\": \"image/jpeg\",\n   *         \"data\": \"/9j/4AAQSkZJRg...\"\n   *       }\n   *     },\n   *     { \"type\": \"text\", \"text\": \"What is in this image?\" }\n   *   ]\n   * }\n   * ```\n   *\n   * We currently support the `base64` source type for images, and the `image/jpeg`,\n   * `image/png`, `image/gif`, and `image/webp` media types.\n   *\n   * See [examples](https://docs.anthropic.com/claude/reference/messages-examples)\n   * for more input examples.\n   *\n   * Note that if you want to include a\n   * [system prompt](https://docs.anthropic.com/claude/docs/system-prompts), you can\n   * use the top-level `system` parameter — there is no `\"system\"` role for input\n   * messages in the Messages API.\n   */\n  messages: Array<MessageParam>;\n\n  /**\n   * The model that will complete your prompt.\n   *\n   * See [models](https://docs.anthropic.com/claude/docs/models-overview) for\n   * additional details and options.\n   */\n  model:\n    | (string & {})\n    | 'claude-3-opus-20240229'\n    | 'claude-3-sonnet-20240229'\n    | 'claude-3-haiku-20240307'\n    | \"claude-2.1'\"\n    | 'claude-2.0'\n    | 'claude-instant-1.2';\n\n  /**\n   * An object describing metadata about the request.\n   */\n  metadata?: MessageCreateParams.Metadata;\n\n  /**\n   * Custom text sequences that will cause the model to stop generating.\n   *\n   * Our models will normally stop when they have naturally completed their turn,\n   * which will result in a response `stop_reason` of `\"end_turn\"`.\n   *\n   * If you want the model to stop generating when it encounters custom strings of\n   * text, you can use the `stop_sequences` parameter. If the model encounters one of\n   * the custom sequences, the response `stop_reason` value will be `\"stop_sequence\"`\n   * and the response `stop_sequence` value will contain the matched stop sequence.\n   */\n  stop_sequences?: Array<string>;\n\n  /**\n   * Whether to incrementally stream the response using server-sent events.\n   *\n   * See [streaming](https://docs.anthropic.com/claude/reference/messages-streaming)\n   * for details.\n   */\n  stream?: boolean;\n\n  /**\n   * System prompt.\n   *\n   * A system prompt is a way of providing context and instructions to Claude, such\n   * as specifying a particular goal or role. See our\n   * [guide to system prompts](https://docs.anthropic.com/claude/docs/system-prompts).\n   */\n  system?: string;\n\n  /**\n   * Amount of randomness injected into the response.\n   *\n   * Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0`\n   * for analytical / multiple choice, and closer to `1.0` for creative and\n   * generative tasks.\n   *\n   * Note that even with `temperature` of `0.0`, the results will not be fully\n   * deterministic.\n   */\n  temperature?: number;\n\n  /**\n   * Only sample from the top K options for each subsequent token.\n   *\n   * Used to remove \"long tail\" low probability responses.\n   * [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n   *\n   * Recommended for advanced use cases only. You usually only need to use\n   * `temperature`.\n   */\n  top_k?: number;\n\n  /**\n   * Use nucleus sampling.\n   *\n   * In nucleus sampling, we compute the cumulative distribution over all the options\n   * for each subsequent token in decreasing probability order and cut it off once it\n   * reaches a particular probability specified by `top_p`. You should either alter\n   * `temperature` or `top_p`, but not both.\n   *\n   * Recommended for advanced use cases only. You usually only need to use\n   * `temperature`.\n   */\n  top_p?: number;\n}\n\nexport namespace MessageCreateParams {\n  /**\n   * An object describing metadata about the request.\n   */\n  export interface Metadata {\n    /**\n     * An external identifier for the user who is associated with the request.\n     *\n     * This should be a uuid, hash value, or other opaque identifier. Anthropic may use\n     * this id to help detect abuse. Do not include any identifying information such as\n     * name, email address, or phone number.\n     */\n    user_id?: string | null;\n  }\n\n  export type MessageCreateParamsNonStreaming = MessagesAPI.MessageCreateParamsNonStreaming;\n  export type MessageCreateParamsStreaming = MessagesAPI.MessageCreateParamsStreaming;\n}\n\nexport interface MessageCreateParamsNonStreaming extends MessageCreateParamsBase {\n  /**\n   * Whether to incrementally stream the response using server-sent events.\n   *\n   * See [streaming](https://docs.anthropic.com/claude/reference/messages-streaming)\n   * for details.\n   */\n  stream?: false;\n}\n\nexport interface MessageCreateParamsStreaming extends MessageCreateParamsBase {\n  /**\n   * Whether to incrementally stream the response using server-sent events.\n   *\n   * See [streaming](https://docs.anthropic.com/claude/reference/messages-streaming)\n   * for details.\n   */\n  stream: true;\n}\n\nexport interface MessageStreamParams {\n  /**\n   * The maximum number of tokens to generate before stopping.\n   *\n   * Note that our models may stop _before_ reaching this maximum. This parameter\n   * only specifies the absolute maximum number of tokens to generate.\n   *\n   * Different models have different maximum values for this parameter. See\n   * [models](https://docs.anthropic.com/claude/docs/models-overview) for details.\n   */\n  max_tokens: number;\n\n  /**\n   * Input messages.\n   *\n   * Our models are trained to operate on alternating `user` and `assistant`\n   * conversational turns. When creating a new `Message`, you specify the prior\n   * conversational turns with the `messages` parameter, and the model then generates\n   * the next `Message` in the conversation.\n   *\n   * Each input message must be an object with a `role` and `content`. You can\n   * specify a single `user`-role message, or you can include multiple `user` and\n   * `assistant` messages. The first message must always use the `user` role.\n   *\n   * If the final message uses the `assistant` role, the response content will\n   * continue immediately from the content in that message. This can be used to\n   * constrain part of the model's response.\n   *\n   * Example with a single `user` message:\n   *\n   * ```json\n   * [{ \"role\": \"user\", \"content\": \"Hello, Claude\" }]\n   * ```\n   *\n   * Example with multiple conversational turns:\n   *\n   * ```json\n   * [\n   *   { \"role\": \"user\", \"content\": \"Hello there.\" },\n   *   { \"role\": \"assistant\", \"content\": \"Hi, I'm Claude. How can I help you?\" },\n   *   { \"role\": \"user\", \"content\": \"Can you explain LLMs in plain English?\" }\n   * ]\n   * ```\n   *\n   * Example with a partially-filled response from Claude:\n   *\n   * ```json\n   * [\n   *   {\n   *     \"role\": \"user\",\n   *     \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"\n   *   },\n   *   { \"role\": \"assistant\", \"content\": \"The best answer is (\" }\n   * ]\n   * ```\n   *\n   * Each input message `content` may be either a single `string` or an array of\n   * content blocks, where each block has a specific `type`. Using a `string` for\n   * `content` is shorthand for an array of one content block of type `\"text\"`. The\n   * following input messages are equivalent:\n   *\n   * ```json\n   * { \"role\": \"user\", \"content\": \"Hello, Claude\" }\n   * ```\n   *\n   * ```json\n   * { \"role\": \"user\", \"content\": [{ \"type\": \"text\", \"text\": \"Hello, Claude\" }] }\n   * ```\n   *\n   * Starting with Claude 3 models, you can also send image content blocks:\n   *\n   * ```json\n   * {\n   *   \"role\": \"user\",\n   *   \"content\": [\n   *     {\n   *       \"type\": \"image\",\n   *       \"source\": {\n   *         \"type\": \"base64\",\n   *         \"media_type\": \"image/jpeg\",\n   *         \"data\": \"/9j/4AAQSkZJRg...\"\n   *       }\n   *     },\n   *     { \"type\": \"text\", \"text\": \"What is in this image?\" }\n   *   ]\n   * }\n   * ```\n   *\n   * We currently support the `base64` source type for images, and the `image/jpeg`,\n   * `image/png`, `image/gif`, and `image/webp` media types.\n   *\n   * See [examples](https://docs.anthropic.com/claude/reference/messages-examples)\n   * for more input examples.\n   *\n   * Note that if you want to include a\n   * [system prompt](https://docs.anthropic.com/claude/docs/system-prompts), you can\n   * use the top-level `system` parameter — there is no `\"system\"` role for input\n   * messages in the Messages API.\n   */\n  messages: Array<MessageParam>;\n\n  /**\n   * The model that will complete your prompt.\n   *\n   * See [models](https://docs.anthropic.com/claude/docs/models-overview) for\n   * additional details and options.\n   */\n  model:\n    | (string & {})\n    | 'claude-3-opus-20240229'\n    | 'claude-3-sonnet-20240229'\n    | 'claude-3-haiku-20240307'\n    | \"claude-2.1'\"\n    | 'claude-2.0'\n    | 'claude-instant-1.2';\n\n  /**\n   * An object describing metadata about the request.\n   */\n  metadata?: MessageStreamParams.Metadata;\n\n  /**\n   * Custom text sequences that will cause the model to stop generating.\n   *\n   * Our models will normally stop when they have naturally completed their turn,\n   * which will result in a response `stop_reason` of `\"end_turn\"`.\n   *\n   * If you want the model to stop generating when it encounters custom strings of\n   * text, you can use the `stop_sequences` parameter. If the model encounters one of\n   * the custom sequences, the response `stop_reason` value will be `\"stop_sequence\"`\n   * and the response `stop_sequence` value will contain the matched stop sequence.\n   */\n  stop_sequences?: Array<string>;\n\n  /**\n   * System prompt.\n   *\n   * A system prompt is a way of providing context and instructions to Claude, such\n   * as specifying a particular goal or role. See our\n   * [guide to system prompts](https://docs.anthropic.com/claude/docs/system-prompts).\n   */\n  system?: string;\n\n  /**\n   * Amount of randomness injected into the response.\n   *\n   * Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0`\n   * for analytical / multiple choice, and closer to `1.0` for creative and\n   * generative tasks.\n   *\n   * Note that even with `temperature` of `0.0`, the results will not be fully\n   * deterministic.\n   */\n  temperature?: number;\n\n  /**\n   * Only sample from the top K options for each subsequent token.\n   *\n   * Used to remove \"long tail\" low probability responses.\n   * [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n   *\n   * Recommended for advanced use cases only. You usually only need to use\n   * `temperature`.\n   */\n  top_k?: number;\n\n  /**\n   * Use nucleus sampling.\n   *\n   * In nucleus sampling, we compute the cumulative distribution over all the options\n   * for each subsequent token in decreasing probability order and cut it off once it\n   * reaches a particular probability specified by `top_p`. You should either alter\n   * `temperature` or `top_p`, but not both.\n   *\n   * Recommended for advanced use cases only. You usually only need to use\n   * `temperature`.\n   */\n  top_p?: number;\n}\n\nexport namespace MessageStreamParams {\n  /**\n   * An object describing metadata about the request.\n   */\n  export interface Metadata {\n    /**\n     * An external identifier for the user who is associated with the request.\n     *\n     * This should be a uuid, hash value, or other opaque identifier. Anthropic may use\n     * this id to help detect abuse. Do not include any identifying information such as\n     * name, email address, or phone number.\n     */\n    user_id?: string | null;\n  }\n}\n\nexport namespace Messages {\n  export import ContentBlock = MessagesAPI.ContentBlock;\n  export import ContentBlockDeltaEvent = MessagesAPI.ContentBlockDeltaEvent;\n  export import ContentBlockStartEvent = MessagesAPI.ContentBlockStartEvent;\n  export import ContentBlockStopEvent = MessagesAPI.ContentBlockStopEvent;\n  export import ImageBlockParam = MessagesAPI.ImageBlockParam;\n  export import Message = MessagesAPI.Message;\n  export import MessageDeltaEvent = MessagesAPI.MessageDeltaEvent;\n  export import MessageDeltaUsage = MessagesAPI.MessageDeltaUsage;\n  export import MessageParam = MessagesAPI.MessageParam;\n  export import MessageStartEvent = MessagesAPI.MessageStartEvent;\n  export import MessageStopEvent = MessagesAPI.MessageStopEvent;\n  export import MessageStreamEvent = MessagesAPI.MessageStreamEvent;\n  export import TextBlock = MessagesAPI.TextBlock;\n  export import TextDelta = MessagesAPI.TextDelta;\n  export import Usage = MessagesAPI.Usage;\n  export import MessageCreateParams = MessagesAPI.MessageCreateParams;\n  export import MessageCreateParamsNonStreaming = MessagesAPI.MessageCreateParamsNonStreaming;\n  export import MessageCreateParamsStreaming = MessagesAPI.MessageCreateParamsStreaming;\n  export import MessageStreamParams = MessagesAPI.MessageStreamParams;\n}\n"],"mappings":";AAAA;SAISA,WAAW,QAAQ,4BAA4B;SAC/CC,aAAa,QAAQ,qCAAqC;SAC1DA,aAAa,QAAQ,qCAAqC;AAInE,OAAM,MAAOC,QAAS,SAAQF,WAAW;EAmBvCG,MAAMA,CACJC,IAAyB,EACzBC,OAA6B;IAAA,IAAAC,YAAA;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,cAAc,EAAAC,aAAA,CAAAA,aAAA;MACrCL,IAAI;MACJM,OAAO,EAAE;IAAM,GACZL,OAAO;MACVM,MAAM,GAAAL,YAAA,GAAEF,IAAI,CAACO,MAAM,cAAAL,YAAA,cAAAA,YAAA,GAAI;IAAK,EAC7B,CAAiE;EACpE;EAEA;;;EAGAK,MAAMA,CAACP,IAAyB,EAAEC,OAA6B;IAC7D,OAAOJ,aAAa,CAACW,aAAa,CAAC,IAAI,EAAER,IAAI,EAAEC,OAAO,CAAC;EACzD;;AAspBF,WAAiBH,QAAQ,GAoBzB,CAAC,EApBgBA,QAAQ,KAARA,QAAQ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}